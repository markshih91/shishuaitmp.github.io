---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: "2018年中科院自动化所模式识别考博真题解答"
slug: "1"
subtitle: ""
summary: ""
authors: [SHISHUAI]
tags: [pattern recognition]
categories: []
date: 2020-03-19T21:35:58+08:00
lastmod: 2020-03-19T21:35:58+08:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ""
  focal_point: ""
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
---

[点击这里下载题目](https://pan.baidu.com/s/1hzoezAQ-zkePQJ7lLt9H2g)，提取码: wbs4

### 1、解
(1) 令$g_{ij}(x)=\ln(\frac{p(x|w_i)}{p(x|w_j)})$$=\ln p(x|w_i)-\ln p(x|w_j)$，则$g_{ij}$可以作为$w_i$和$w_j$两个类别的判别函数，如果$g_{ij}(x)\ge0$，则$x\in w_i$，否则$x\in w_j$. （其中 $i,j = 1,2,\dots, c$）

(2) 对(1)中的判别函数进行化简得
<div>
$$
\begin{aligned}
g_{ij}(x)= & \ln\frac{p(x|w_i)}{p(x|w_j)} = \ln p(x|w_i)-\ln p(x|w_j) \\
= & \ln\{\frac{1}{(2\pi)^\frac{d}{2}|\Sigma_i|^\frac{1}{2}}\exp(-\frac{1}{2}(x-u_i)^T\Sigma_i^{-1}(x-u_i))\} \\
& - \ln\{\frac{1}{(2\pi)^\frac{d}{2}|\Sigma_j|^\frac{1}{2}}\exp(-\frac{1}{2}(x-u_j)^T\Sigma_j^{-1}(x-u_j))\} \\
= & -\frac{1}{2}\{(x-u_i)^T\Sigma_i^{-1}(x-u_i) - (x-u_j)^T\Sigma_j^{-1}(x-u_j)\}-\frac{1}{2}\ln\frac{|\Sigma_i|}{|\Sigma_j|} \\
= & -\frac{1}{2}(x^T\Sigma_i^{-1}x - x^T\Sigma_j^{-1}x) + (u_i^T\Sigma_i^{-1}-u_j^T\Sigma_j^{-1})x-\frac{1}{2}(u_i^T\Sigma_i^{-1}u_i - u_j^T\Sigma_j^{-1}u_j) \\
& - \frac{1}{2}\ln\frac{|\Sigma_i|}{|\Sigma_j|}
\end{aligned}
$$
</div>
显然，当$\Sigma_i=\Sigma_j$时，$g_{ij}$为线性判别函数，因此当各类条件概率密度函数中的协方差相同时，为线性判别函数。

### 2、解
(1) Parzen窗概率密度估计函数为
$$p(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(x-x_i)^2}{2\sigma^2})$$
当$\sigma=0.5$时
$$p(x)=\sqrt{\frac{2}{\pi}}\exp(-2(x-x_i)^2)$$
其中$x_i$为密度函数均值

(2) 
概率密度函数为
<div>
$$p(x)=\left\{
\begin{aligned}
\frac{1}{8|x-8|} & , & x>6 \\
\frac{1}{8|x-4|} & , & 2< x\le6 \\
\frac{1}{8|x|} & , & -1< x\le2 \\
\frac{1}{8|x+2|} & , & x\le-1
\end{aligned}
\right.
$$
</div>
曲线图为

 <center><img src="/img/20200319-CASIA-pr2018/2018-2-2.png" width="50%"></center>

 ### 3、解
 (1) 算法步骤：
 
1) 确定聚类个数$c$，随机选择$c$个聚类中心;

2) 对每个样本点，选择与其欧式距离最近的聚类中心作为其类别代表，将具有相同类别代表的样本归为一类，划分为$c$个类，计算每个类的样本均值$m_i$，以$m_i$作为每类新的聚类中心，$i=1,\dots,c$;

3) 对于第$i$类的一个样本$x'$，计算将其放入$j$类之后，总的平方误差的减少量$I_{ij}$
$$I_{ij}=\frac{n_i}{n_i-1}||x'-m_i||^2-\frac{n_j}{n_j+1}||x'-m_j||^2$$
令$I_{ik}=\mathop{max}\limits_j\{I_{ij}\}$，如果$I_{ik}>0$，则将$x'$从第$i$个类别移动到第$k$个类别，更新$i$类和$k$类均值
$$m_i \leftarrow m_i-\frac{1}{n_i-1}(x'-m_i)$$
$$m_k \leftarrow m_k+\frac{1}{n_k+1}(x'-m_k)$$
否则不移动$x'$;

4) 重复2.和3.过程，如果移动任何样本都不会减少总的平方误差，则算法结束。
   
(2) 假设将$x'$移动到$D_j$聚类之后，$D_j$聚类新的均值为
<div>$$m_j^*=m_j+\frac{1}{n_j+1}(x'-m_j)$$</div>
$D_j$聚类新的平方误差为
$$
\begin{aligned}
J_j^* =& \sum_{x\in D_j\cup {x'}}||x-m_j^*||^2 \\
=& \sum_{x\in D_j}||x-m_j^*||^2+||x'-m_j^*||^2 \\
=& \sum_{x\in D_j}||x-m_j-\frac{1}{n_j+1}(x'-m_j)||^2+||x'-m_j-\frac{1}{n_j+1}(x'-m_j)||^2 \\
=& \sum_{x\in D_j}\left\{||x-m_j||^2 + ||\frac{1}{n_j+1}(x'-m_j)||^2-\frac{2(x-m_j)^T(x'-m_j)}{n_j+1}\right\} \\
& +\frac{n_j^2}{(n_j+1)^2}||x'-m_j||^2 \\
=& J_j + \frac{n_j}{(n_j+1)^2}||x'-m_j||^2+\frac{n_j^2}{(n_j+1)^2}||x'-m_j||^2 \\
=& J_j + \frac{n_j}{n_j+1}||x'-m_j||^2
\end{aligned}
$$

### 4、解
(1) Adaboost算法步骤：

1) 选择m个弱分类器$f_j(x)\in \{-1, 1\}$, 如果$x$为第一类样本，则结果为$1$，否则结果为$-1$, $j=1,\dots,m$；
2) 初始化样本权重$w_i=\frac{1}{n}$, $i=1,\dots,n$；
3) 令$j=1\to m$，作如下操作：
   1) 使用$n$个由$w_i$加权的样本$x_i$构造分类器$f_j$，$i=1,\dots,n$；
   2) 计算$f_j$在训练样本上的分类错误率$e_j$，令$c_j=log\frac{1-e_j}{e_j}$. 如果$f_j$对$x_i$样本分类正确，则权重$w_i$不变，否则$w_i=w_i\cdot exp(c_j)$，$i=1,\dots,n$；
   3) 将$w_i$归一化，$i=1,\dots,n$.
4) 构造最终分类器$f(x)=\mathrm{sgn}\left[\sum_{j=1}^m c_jf_j(x)\right]$.
   
(2) 训练误差为0后，Adaboost会继续增大分类间距，提升模型的泛化能力，减少测试误差

### 5、解
(1) K-L变换矩阵为
<div>
$$
\begin{aligned}
\Sigma 
=& E[x\cdot x^T] \\
=& \frac{1}{8}\{(-4, 1)^T (-4, 1) + (-2, 1)^T (-2, 1) \\
=& (-4, -1)^T (-4, -1) + (-2, -1)^T (-2, -1) \\
=& (4, 1)^T (4, 1) + (2, 1)^T (2, 1) \\
=& (4, -1)^T (4, -1) + (2, -1)^T (2, -1)\} \\
=& \begin{gathered}
\begin{pmatrix} 10 & 0 \\ 0 & 1 \end{pmatrix}
\end{gathered}
\end{aligned}
$$
</div>
显然，$\Sigma$的特征值为$10$和$1$，对应的特征向量分别为$(1, 0)^T$和$(0, 1)^T$.

(2) 令两类集合分别为$w_1$和$w_2$, 各类样本均值分别为$m_1$和$m_2$，则
<div>
$$
\begin{aligned}
& S_b=(m_1-m_2)(m_1-m_2)^T \\
& S_w=\sum_{x\in w_1}(x-m_1)(x-m_1)^T+\sum_{x\in w_2}(x-m_2)(x-m_2)^T
\end{aligned}
$$
</div>
对第一个特征计算样本的$S_b$和$S_w$得
$$S_b=36, S_w=8$$
因此$J_5=\frac{11}{2}$
对第二个特征计算样本的$S_b$和$S_w$得
$$S_b=0, S_w=8$$
因此$J_5=0$. 由于$\frac{11}{2}>0$，所以应选择第一个特征进行分类任务。

### 6、解
(1) 原优化问题：
<div>
$$
\begin{aligned}
& \min_{W,b} & \frac{1}{2}||W||^2+C\sum_{i=1}^n \xi_i\\
& s.t. & y_i(W^Tx_i+b)\ge1-\xi_i, \\
& & \xi_i\ge0, i=1,\dots,n.
\end{aligned}
$$
</div>
对偶优化问题：
<div>
$$
\begin{aligned}
& \max_{\alpha} & \sum_{i=1}^n\alpha_i-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^na_ia_jy_iy_jx_i^Tx_j\\
& s.t. & \sum_{i=1}^na_iy_i=0, \\
& & 0\le\alpha_i\le C, i=1,\dots,n.
\end{aligned}
$$
</div>

(2) 当$C$很大时，分类超平面对错分样本比较敏感，因此要保证不错分的情况下，最大化最小间隔
 <center><img src="/img/20200319-CASIA-pr2018/2018-6-2.jpg" width="50%"></center>

(3) 当$C$很小时，分类超平面对错分样本不敏感，而对$W$的长度比较敏感
 <center><img src="/img/20200319-CASIA-pr2018/2018-6-3.jpg" width="50%"></center>

 ### 7、解
(1) 假设输入层神经元个数为$d$个，每个神经元的输入为$x_i$，$i=1,\dots,d$；隐藏层的神经元个数为$d_1$个，激活值分别为$h'_j$，输出分别为$h_j=\sigma_1(h'_i)$，其中$\sigma_1$为隐藏层的激活函数，$j=1,\dots,d_1$；输出层的神经元个数为$c$个，激活值分别为$y'_k$，输出为$y_k=\sigma_2(y'_k)$，其中$\sigma_2$为输出层的激活函数，$k=1,\dots,c$. 
 
输入层与隐藏层之间的参数为$w^{(1)}_{ij}$和$b^{(1)}_j$，满足：
<div>
$$h'_j=\sum_{i=1}^dw^{(1)}_{ij}x_i+b^{(1)}_j$$
</div>

输入层与隐藏层之间的参数为$w^{(2)}_{jk}$和$b^{(2)}_k$，满足：
<div>
$$y'_k=\sum_{j=1}^dw^{(2)}_{jk}h_j+b^{(2)}_k$$
</div>

令$L$为网络的损失函数，则有：
<div>
$$\left\{
\begin{aligned}
& \nabla_{w^{(2)}_{jk}}=\sum_{k=1}^c\frac{\partial L}{\partial y_k}\frac{\partial y_k}{\partial y'_k}\frac{\partial y'_k}{\partial w^{(2)}_{jk}} = \sum_{k=1}^c\frac{\partial L}{\partial y_k}\sigma'_2(y'_k)h_j \\
& \nabla_{b^{(2)}_k}=\sum_{k=1}^c\frac{\partial L}{\partial y_k}\frac{\partial y_k}{\partial y'_k}\frac{\partial y'_k}{\partial b^{(2)}_k} = \sum_{k=1}^c\frac{\partial L}{\partial y_k}\sigma'_2(y'_k) \\
& \nabla_{w^{(1)}_{ij}}=\sum_{k=1}^c\sum_{j=1}^{d_1}\frac{\partial L}{\partial y_k}\frac{\partial y_k}{\partial y'_k}\frac{\partial y'_k}{\partial h_j}\frac{\partial h_j}{\partial h'_j}\frac{\partial h'_j}{\partial w^{(1)}_{ij}} = \sum_{k=1}^c\sum_{j=1}^{d_1}\frac{\partial L}{\partial y_k}\sigma'_2(y'_k)w^{(2)}_{jk}\sigma'_1(h'_j)x_i\\
& \nabla_{b^{(1)}_j}=\sum_{k=1}^c\sum_{j=1}^{d_1}\frac{\partial L}{\partial y_k}\frac{\partial y_k}{\partial y'_k}\frac{\partial y'_k}{\partial h_j}\frac{\partial h_j}{\partial h'_j}\frac{\partial h'_j}{\partial b^{(1)}_j} = \sum_{k=1}^c\sum_{j=1}^{d_1}\frac{\partial L}{\partial y_k}\sigma'_2(y'_k)w^{(2)}_{jk}\sigma'_1(h'_j)
\end{aligned}
\right.
$$
</div>

网络经过前向传播之后，得到了计算梯度$\nabla_{w^{(2)}_{jk}}$, $\nabla_{b^{(2)}_k}$, $\nabla_{w^{(1)}_{ij}}$, $\nabla_{b^{(1)}_j}$所需的各个变量的值，因此可以利用反向传播依次求出它们的值，然后利用梯度下降对参数$w^{(2)}_{jk}$, $b^{(2)}_k$, $w^{(1)}_{ij}$, $b^{(1)}_j$进行更新：
<div>
$$\left\{
\begin{aligned}
& w^{(2)}_{jk}\gets w^{(2)}_{jk} - \eta\nabla_{w^{(2)}_{jk}} \\
& b^{(2)}_k\gets b^{(2)}_k - \eta\nabla_{b^{(2)}_k} \\
& w^{(1)}_{ij}\gets w^{(1)}_{ij} - \eta\nabla_{w^{(1)}_{ij}} \\
& b^{(1)}_j\gets b^{(1)}_j - \eta\nabla_{b^{(1)}_j}
\end{aligned}
\right.
$$
</div>
其中$\eta$为学习率。
<div>
(2) 沿用(1)中的假设，计算步骤为：
<ol>
<li> 初始化参数$w^{(2)}_{jk}$，$b^{(2)}_k$，$w^{(1)}_{ij}$, $b^{(1)}_j$;</li>
<li> 利用前向传播过程得到输出，从输入$x_i$，得到$h_j$和$y_k$，$i=1,\dots,d$，$j=1,\dots,d_1$，$k=1,\dots,c$；</li>
<li> 计算样本标签和输出$y_k$之间的差异，即损失函数$L$；</li>
<li> 参数更新，如果模型损失函数小于阈值，则结束。否则，利用(1)中公式更新参数$w^{(2)}_{jk}$, $b^{(2)}_k$, $w^{(1)}_{ij}$, $b^{(1)}_j$的值；</li>
<li> 重复1,2,3,4，直至结束。</li>
</ol>
</div>
(3) 如果神经网络中的激活函数为Sigmoid函数，以$\sigma_1$为例，即
$$
\sigma_1(x)=\frac{1}{1+\exp(-x)}
$$
则
<div>
$$
\sigma'_1(x)=\frac{\exp(-x)}{(1+\exp(-x))^2}=\sigma_1(x)(1-\sigma_1(x))
$$
</div>
当x远离$0$时，$\sigma'_1(x)\to 0$，从而$\nabla_{w^{(1)}_{ij}}\to 0$, $\nabla_{b^{(1)}_j}\to 0$，这样参数$w^{(1)}_{ij}$, $b^{(1)}_j$在训练过程中得不到有效更新，因此出现麻痹现象。

### 8、解
(1) 深度学习定义：深度学习是机器学习算法的一种，通过创建多层神经网络模型，使其能够自动提取样本中的特征，并对样本进行分类或回归，采用反向传播算法对参数进行更新，从而得到对样本有效分类或回归的模型。

(2) 
 1) 卷积神经网络(CNN)：图像分类、目标检测；
 2) 递归神经网络(RNN)：机器翻译、人机对话；
 3) 全卷积神经网路：语义分割；
 4) 全连接神经网络：邮件分类