<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>pattern recognition | SHUAI SHI</title>
    <link>https://shishuai.org/tags/pattern-recognition/</link>
      <atom:link href="https://shishuai.org/tags/pattern-recognition/index.xml" rel="self" type="application/rss+xml" />
    <description>pattern recognition</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 19 Mar 2020 21:35:58 +0800</lastBuildDate>
    <image>
      <url>https://shishuai.org/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>pattern recognition</title>
      <link>https://shishuai.org/tags/pattern-recognition/</link>
    </image>
    
    <item>
      <title>2018年中科院自动化所模式识别考博真题解答</title>
      <link>https://shishuai.org/post/2020/03/19/1/</link>
      <pubDate>Thu, 19 Mar 2020 21:35:58 +0800</pubDate>
      <guid>https://shishuai.org/post/2020/03/19/1/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://pan.baidu.com/s/1hzoezAQ-zkePQJ7lLt9H2g&#34;&gt;点击这里下载题目&lt;/a&gt;，提取码: wbs4&lt;/p&gt;
&lt;h3 id=&#34;1解&#34;&gt;1、解&lt;/h3&gt;
&lt;p&gt;(1) 令$g_{ij}(x)=\ln(\frac{p(x|w_i)}{p(x|w_j)})$$=\ln p(x|w_i)-\ln p(x|w_j)$，则$g_{ij}$可以作为$w_i$和$w_j$两个类别的判别函数，如果$g_{ij}(x)\ge0$，则$x\in w_i$，否则$x\in w_j$. （其中 $i,j = 1,2,\dots, c$）&lt;/p&gt;
&lt;p&gt;(2) 对(1)中的判别函数进行化简得&lt;/p&gt;
&lt;div&gt;
$$
\begin{aligned}
g_{ij}(x)= &amp; \ln\frac{p(x|w_i)}{p(x|w_j)} = \ln p(x|w_i)-\ln p(x|w_j) \\
= &amp; \ln\{\frac{1}{(2\pi)^\frac{d}{2}|\Sigma_i|^\frac{1}{2}}\exp(-\frac{1}{2}(x-u_i)^T\Sigma_i^{-1}(x-u_i))\} \\
&amp; - \ln\{\frac{1}{(2\pi)^\frac{d}{2}|\Sigma_j|^\frac{1}{2}}\exp(-\frac{1}{2}(x-u_j)^T\Sigma_j^{-1}(x-u_j))\} \\
= &amp; -\frac{1}{2}\{(x-u_i)^T\Sigma_i^{-1}(x-u_i) - (x-u_j)^T\Sigma_j^{-1}(x-u_j)\}-\frac{1}{2}\ln\frac{|\Sigma_i|}{|\Sigma_j|} \\
= &amp; -\frac{1}{2}(x^T\Sigma_i^{-1}x - x^T\Sigma_j^{-1}x) + (u_i^T\Sigma_i^{-1}-u_j^T\Sigma_j^{-1})x-\frac{1}{2}(u_i^T\Sigma_i^{-1}u_i - u_j^T\Sigma_j^{-1}u_j) \\
&amp; - \frac{1}{2}\ln\frac{|\Sigma_i|}{|\Sigma_j|}
\end{aligned}
$$
&lt;/div&gt;
显然，当$\Sigma_i=\Sigma_j$时，$g_{ij}$为线性判别函数，因此当各类条件概率密度函数中的协方差相同时，为线性判别函数。
&lt;h3 id=&#34;2解&#34;&gt;2、解&lt;/h3&gt;
&lt;p&gt;(1) Parzen窗概率密度估计函数为
$$p(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(x-x_i)^2}{2\sigma^2})$$
当$\sigma=0.5$时
$$p(x)=\sqrt{\frac{2}{\pi}}\exp(-2(x-x_i)^2)$$
其中$x_i$为密度函数均值&lt;/p&gt;
&lt;p&gt;(2)
概率密度函数为&lt;/p&gt;
&lt;div&gt;
$$p(x)=\left\{
\begin{aligned}
\frac{1}{8|x-8|} &amp; , &amp; x&gt;6 \\
\frac{1}{8|x-4|} &amp; , &amp; 2&lt; x\le6 \\
\frac{1}{8|x|} &amp; , &amp; -1&lt; x\le2 \\
\frac{1}{8|x+2|} &amp; , &amp; x\le-1
\end{aligned}
\right.
$$
&lt;/div&gt;
曲线图为
 &lt;center&gt;&lt;img src=&#34;https://shishuai.org/img/20200319-CASIA-pr2018/2018-2-2.png&#34; width=&#34;50%&#34;&gt;&lt;/center&gt;
&lt;h3 id=&#34;3解&#34;&gt;3、解&lt;/h3&gt;
&lt;p&gt;(1) 算法步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;确定聚类个数$c$，随机选择$c$个聚类中心;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对每个样本点，选择与其欧式距离最近的聚类中心作为其类别代表，将具有相同类别代表的样本归为一类，划分为$c$个类，计算每个类的样本均值$m_i$，以$m_i$作为每类新的聚类中心，$i=1,\dots,c$;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对于第$i$类的一个样本$x&#39;$，计算将其放入$j$类之后，总的平方误差的减少量$I_{ij}$
$$I_{ij}=\frac{n_i}{n_i-1}||x&amp;rsquo;-m_i||^2-\frac{n_j}{n_j+1}||x&amp;rsquo;-m_j||^2$$
令$I_{ik}=\mathop{max}\limits_j{I_{ij}}$，如果$I_{ik}&amp;gt;0$，则将$x&#39;$从第$i$个类别移动到第$k$个类别，更新$i$类和$k$类均值
$$m_i \leftarrow m_i-\frac{1}{n_i-1}(x&amp;rsquo;-m_i)$$
$$m_k \leftarrow m_k+\frac{1}{n_k+1}(x&amp;rsquo;-m_k)$$
否则不移动$x&#39;$;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;重复2.和3.过程，如果移动任何样本都不会减少总的平方误差，则算法结束。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(2) 假设将$x&#39;$移动到$D_j$聚类之后，$D_j$聚类新的均值为&lt;/p&gt;
&lt;div&gt;$$m_j^*=m_j+\frac{1}{n_j+1}(x&#39;-m_j)$$&lt;/div&gt;
$D_j$聚类新的平方误差为
$$
\begin{aligned}
J_j^* =&amp; \sum_{x\in D_j\cup {x&#39;}}||x-m_j^*||^2 \\
=&amp; \sum_{x\in D_j}||x-m_j^*||^2+||x&#39;-m_j^*||^2 \\
=&amp; \sum_{x\in D_j}||x-m_j-\frac{1}{n_j+1}(x&#39;-m_j)||^2+||x&#39;-m_j-\frac{1}{n_j+1}(x&#39;-m_j)||^2 \\
=&amp; \sum_{x\in D_j}\left\{||x-m_j||^2 + ||\frac{1}{n_j+1}(x&#39;-m_j)||^2-\frac{2(x-m_j)^T(x&#39;-m_j)}{n_j+1}\right\} \\
&amp; +\frac{n_j^2}{(n_j+1)^2}||x&#39;-m_j||^2 \\
=&amp; J_j + \frac{n_j}{(n_j+1)^2}||x&#39;-m_j||^2+\frac{n_j^2}{(n_j+1)^2}||x&#39;-m_j||^2 \\
=&amp; J_j + \frac{n_j}{n_j+1}||x&#39;-m_j||^2
\end{aligned}
$$
&lt;h3 id=&#34;4解&#34;&gt;4、解&lt;/h3&gt;
&lt;p&gt;(1) Adaboost算法步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;选择m个弱分类器$f_j(x)\in {-1, 1}$, 如果$x$为第一类样本，则结果为$1$，否则结果为$-1$, $j=1,\dots,m$；&lt;/li&gt;
&lt;li&gt;初始化样本权重$w_i=\frac{1}{n}$, $i=1,\dots,n$；&lt;/li&gt;
&lt;li&gt;令$j=1\to m$，作如下操作：
&lt;ol&gt;
&lt;li&gt;使用$n$个由$w_i$加权的样本$x_i$构造分类器$f_j$，$i=1,\dots,n$；&lt;/li&gt;
&lt;li&gt;计算$f_j$在训练样本上的分类错误率$e_j$，令$c_j=log\frac{1-e_j}{e_j}$. 如果$f_j$对$x_i$样本分类正确，则权重$w_i$不变，否则$w_i=w_i\cdot exp(c_j)$，$i=1,\dots,n$；&lt;/li&gt;
&lt;li&gt;将$w_i$归一化，$i=1,\dots,n$.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;构造最终分类器$f(x)=\mathrm{sgn}\left[\sum_{j=1}^m c_jf_j(x)\right]$.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;(2) 训练误差为0后，Adaboost会继续增大分类间距，提升模型的泛化能力，减少测试误差&lt;/p&gt;
&lt;h3 id=&#34;5解&#34;&gt;5、解&lt;/h3&gt;
&lt;p&gt;(1) K-L变换矩阵为&lt;/p&gt;
&lt;div&gt;
$$
\begin{aligned}
\Sigma 
=&amp; E[x\cdot x^T] \\
=&amp; \frac{1}{8}\{(-4, 1)^T (-4, 1) + (-2, 1)^T (-2, 1) \\
=&amp; (-4, -1)^T (-4, -1) + (-2, -1)^T (-2, -1) \\
=&amp; (4, 1)^T (4, 1) + (2, 1)^T (2, 1) \\
=&amp; (4, -1)^T (4, -1) + (2, -1)^T (2, -1)\} \\
=&amp; \begin{gathered}
\begin{pmatrix} 10 &amp; 0 \\ 0 &amp; 1 \end{pmatrix}
\end{gathered}
\end{aligned}
$$
&lt;/div&gt;
显然，$\Sigma$的特征值为$10$和$1$，对应的特征向量分别为$(1, 0)^T$和$(0, 1)^T$.
&lt;p&gt;(2) 令两类集合分别为$w_1$和$w_2$, 各类样本均值分别为$m_1$和$m_2$，则&lt;/p&gt;
&lt;div&gt;
$$
\begin{aligned}
&amp; S_b=(m_1-m_2)(m_1-m_2)^T \\
&amp; S_w=\sum_{x\in w_1}(x-m_1)(x-m_1)^T+\sum_{x\in w_2}(x-m_2)(x-m_2)^T
\end{aligned}
$$
&lt;/div&gt;
对第一个特征计算样本的$S_b$和$S_w$得
$$S_b=36, S_w=8$$
因此$J_5=\frac{11}{2}$
对第二个特征计算样本的$S_b$和$S_w$得
$$S_b=0, S_w=8$$
因此$J_5=0$. 由于$\frac{11}{2}&gt;0$，所以应选择第一个特征进行分类任务。
&lt;h3 id=&#34;6解&#34;&gt;6、解&lt;/h3&gt;
&lt;p&gt;(1) 原优化问题：&lt;/p&gt;
&lt;div&gt;
$$
\begin{aligned}
&amp; \min_{W,b} &amp; \frac{1}{2}||W||^2+C\sum_{i=1}^n \xi_i\\
&amp; s.t. &amp; y_i(W^Tx_i+b)\ge1-\xi_i, \\
&amp; &amp; \xi_i\ge0, i=1,\dots,n.
\end{aligned}
$$
&lt;/div&gt;
对偶优化问题：
&lt;div&gt;
$$
\begin{aligned}
&amp; \max_{\alpha} &amp; \sum_{i=1}^n\alpha_i-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^na_ia_jy_iy_jx_i^Tx_j\\
&amp; s.t. &amp; \sum_{i=1}^na_iy_i=0, \\
&amp; &amp; 0\le\alpha_i\le C, i=1,\dots,n.
\end{aligned}
$$
&lt;/div&gt;
&lt;p&gt;(2) 当$C$很大时，分类超平面对错分样本比较敏感，因此要保证不错分的情况下，最大化最小间隔&lt;/p&gt;
 &lt;center&gt;&lt;img src=&#34;https://shishuai.org/img/20200319-CASIA-pr2018/2018-6-2.jpg&#34; width=&#34;50%&#34;&gt;&lt;/center&gt;
&lt;p&gt;(3) 当$C$很小时，分类超平面对错分样本不敏感，而对$W$的长度比较敏感&lt;/p&gt;
 &lt;center&gt;&lt;img src=&#34;https://shishuai.org/img/20200319-CASIA-pr2018/2018-6-3.jpg&#34; width=&#34;50%&#34;&gt;&lt;/center&gt;
&lt;h3 id=&#34;7解&#34;&gt;7、解&lt;/h3&gt;
&lt;p&gt;(1) 假设输入层神经元个数为$d$个，每个神经元的输入为$x_i$，$i=1,\dots,d$；隐藏层的神经元个数为$d_1$个，激活值分别为$h&amp;rsquo;_j$，输出分别为$h_j=\sigma_1(h&amp;rsquo;_i)$，其中$\sigma_1$为隐藏层的激活函数，$j=1,\dots,d_1$；输出层的神经元个数为$c$个，激活值分别为$y&amp;rsquo;_k$，输出为$y_k=\sigma_2(y&amp;rsquo;_k)$，其中$\sigma_2$为输出层的激活函数，$k=1,\dots,c$.&lt;/p&gt;
&lt;p&gt;输入层与隐藏层之间的参数为$w^{(1)}_{ij}$和$b^{(1)}_j$，满足：&lt;/p&gt;
&lt;div&gt;
$$h&#39;_j=\sum_{i=1}^dw^{(1)}_{ij}x_i+b^{(1)}_j$$
&lt;/div&gt;
&lt;p&gt;输入层与隐藏层之间的参数为$w^{(2)}_{jk}$和$b^{(2)}_k$，满足：&lt;/p&gt;
&lt;div&gt;
$$y&#39;_k=\sum_{j=1}^dw^{(2)}_{jk}h_j+b^{(2)}_k$$
&lt;/div&gt;
&lt;p&gt;令$L$为网络的损失函数，则有：&lt;/p&gt;
&lt;div&gt;
$$\left\{
\begin{aligned}
&amp; \nabla_{w^{(2)}_{jk}}=\sum_{k=1}^c\frac{\partial L}{\partial y_k}\frac{\partial y_k}{\partial y&#39;_k}\frac{\partial y&#39;_k}{\partial w^{(2)}_{jk}} = \sum_{k=1}^c\frac{\partial L}{\partial y_k}\sigma&#39;_2(y&#39;_k)h_j \\
&amp; \nabla_{b^{(2)}_k}=\sum_{k=1}^c\frac{\partial L}{\partial y_k}\frac{\partial y_k}{\partial y&#39;_k}\frac{\partial y&#39;_k}{\partial b^{(2)}_k} = \sum_{k=1}^c\frac{\partial L}{\partial y_k}\sigma&#39;_2(y&#39;_k) \\
&amp; \nabla_{w^{(1)}_{ij}}=\sum_{k=1}^c\sum_{j=1}^{d_1}\frac{\partial L}{\partial y_k}\frac{\partial y_k}{\partial y&#39;_k}\frac{\partial y&#39;_k}{\partial h_j}\frac{\partial h_j}{\partial h&#39;_j}\frac{\partial h&#39;_j}{\partial w^{(1)}_{ij}} = \sum_{k=1}^c\sum_{j=1}^{d_1}\frac{\partial L}{\partial y_k}\sigma&#39;_2(y&#39;_k)w^{(2)}_{jk}\sigma&#39;_1(h&#39;_j)x_i\\
&amp; \nabla_{b^{(1)}_j}=\sum_{k=1}^c\sum_{j=1}^{d_1}\frac{\partial L}{\partial y_k}\frac{\partial y_k}{\partial y&#39;_k}\frac{\partial y&#39;_k}{\partial h_j}\frac{\partial h_j}{\partial h&#39;_j}\frac{\partial h&#39;_j}{\partial b^{(1)}_j} = \sum_{k=1}^c\sum_{j=1}^{d_1}\frac{\partial L}{\partial y_k}\sigma&#39;_2(y&#39;_k)w^{(2)}_{jk}\sigma&#39;_1(h&#39;_j)
\end{aligned}
\right.
$$
&lt;/div&gt;
&lt;p&gt;网络经过前向传播之后，得到了计算梯度$\nabla_{w^{(2)}_{jk}}$, $\nabla_{b^{(2)}_k}$, $\nabla_{w^{(1)}_{ij}}$, $\nabla_{b^{(1)}_j}$所需的各个变量的值，因此可以利用反向传播依次求出它们的值，然后利用梯度下降对参数$w^{(2)}_{jk}$, $b^{(2)}_k$, $w^{(1)}_{ij}$, $b^{(1)}_j$进行更新：&lt;/p&gt;
&lt;div&gt;
$$\left\{
\begin{aligned}
&amp; w^{(2)}_{jk}\gets w^{(2)}_{jk} - \eta\nabla_{w^{(2)}_{jk}} \\
&amp; b^{(2)}_k\gets b^{(2)}_k - \eta\nabla_{b^{(2)}_k} \\
&amp; w^{(1)}_{ij}\gets w^{(1)}_{ij} - \eta\nabla_{w^{(1)}_{ij}} \\
&amp; b^{(1)}_j\gets b^{(1)}_j - \eta\nabla_{b^{(1)}_j}
\end{aligned}
\right.
$$
&lt;/div&gt;
其中$\eta$为学习率。
&lt;div&gt;
(2) 沿用(1)中的假设，计算步骤为：
&lt;ol&gt;
&lt;li&gt; 初始化参数$w^{(2)}_{jk}$，$b^{(2)}_k$，$w^{(1)}_{ij}$, $b^{(1)}_j$;&lt;/li&gt;
&lt;li&gt; 利用前向传播过程得到输出，从输入$x_i$，得到$h_j$和$y_k$，$i=1,\dots,d$，$j=1,\dots,d_1$，$k=1,\dots,c$；&lt;/li&gt;
&lt;li&gt; 计算样本标签和输出$y_k$之间的差异，即损失函数$L$；&lt;/li&gt;
&lt;li&gt; 参数更新，如果模型损失函数小于阈值，则结束。否则，利用(1)中公式更新参数$w^{(2)}_{jk}$, $b^{(2)}_k$, $w^{(1)}_{ij}$, $b^{(1)}_j$的值；&lt;/li&gt;
&lt;li&gt; 重复1,2,3,4，直至结束。&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
(3) 如果神经网络中的激活函数为Sigmoid函数，以$\sigma_1$为例，即
$$
\sigma_1(x)=\frac{1}{1+\exp(-x)}
$$
则
&lt;div&gt;
$$
\sigma&#39;_1(x)=\frac{\exp(-x)}{(1+\exp(-x))^2}=\sigma_1(x)(1-\sigma_1(x))
$$
&lt;/div&gt;
当x远离$0$时，$\sigma&#39;_1(x)\to 0$，从而$\nabla_{w^{(1)}_{ij}}\to 0$, $\nabla_{b^{(1)}_j}\to 0$，这样参数$w^{(1)}_{ij}$, $b^{(1)}_j$在训练过程中得不到有效更新，因此出现麻痹现象。
&lt;h3 id=&#34;8解&#34;&gt;8、解&lt;/h3&gt;
&lt;p&gt;(1) 深度学习定义：深度学习是机器学习算法的一种，通过创建多层神经网络模型，使其能够自动提取样本中的特征，并对样本进行分类或回归，采用反向传播算法对参数进行更新，从而得到对样本有效分类或回归的模型。&lt;/p&gt;
&lt;p&gt;(2)&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;卷积神经网络(CNN)：图像分类、目标检测；&lt;/li&gt;
&lt;li&gt;递归神经网络(RNN)：机器翻译、人机对话；&lt;/li&gt;
&lt;li&gt;全卷积神经网路：语义分割；&lt;/li&gt;
&lt;li&gt;全连接神经网络：邮件分类&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
