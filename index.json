[{"authors":["admin"],"categories":null,"content":"I`m a Researcher of Computer Vision and Artificial Intelligence at Juefx Technology Co., Ltd. My research interests are Computer Vision, Machine Learning and Deep Learning. I'm also interested in more applied problems with nice theoretical components in Mathematices. Here are some of my favorite site/blog links.\nI obtained my M.S. in Pure Mathematics from AMSS, Chinese Academy of Sciences in May 2019. I finished my undergrad in Information and Computing Science at Chongqing University of Technology in 2013, and won several Competition Awards in Mathematics. Here is my Detailed CV.\nI maintain a github account and a blog.\nYou can contact me through email markshih91@gmail.com.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://shishuai.org/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I`m a Researcher of Computer Vision and Artificial Intelligence at Juefx Technology Co., Ltd. My research interests are Computer Vision, Machine Learning and Deep Learning. I'm also interested in more applied problems with nice theoretical components in Mathematices. Here are some of my favorite site/blog links. I obtained my M.S. in Pure Mathematics from AMSS, Chinese Academy of Sciences in May 2019. I finished my undergrad in Information and Computing","tags":null,"title":"Shuai Shi | 史帅","type":"authors"},{"authors":["SHISHUAI"],"categories":[],"content":"点击这里下载题目，提取码: wbs4\n1、解 (1) 令$g_{ij}(x)=\\ln(\\frac{p(x|w_i)}{p(x|w_j)})$$=\\ln p(x|w_i)-\\ln p(x|w_j)$，则$g_{ij}$可以作为$w_i$和$w_j$两个类别的判别函数，如果$g_{ij}(x)\\ge0$，则$x\\in w_i$，否则$x\\in w_j$. （其中 $i,j = 1,2,\\dots, c$）\n(2) 对(1)中的判别函数进行化简得\n $$ \\begin{aligned} g_{ij}(x)= \u0026 \\ln\\frac{p(x|w_i)}{p(x|w_j)} = \\ln p(x|w_i)-\\ln p(x|w_j) \\\\ = \u0026 \\ln\\{\\frac{1}{(2\\pi)^\\frac{d}{2}|\\Sigma_i|^\\frac{1}{2}}\\exp(-\\frac{1}{2}(x-u_i)^T\\Sigma_i^{-1}(x-u_i))\\} \\\\ \u0026 - \\ln\\{\\frac{1}{(2\\pi)^\\frac{d}{2}|\\Sigma_j|^\\frac{1}{2}}\\exp(-\\frac{1}{2}(x-u_j)^T\\Sigma_j^{-1}(x-u_j))\\} \\\\ = \u0026 -\\frac{1}{2}\\{(x-u_i)^T\\Sigma_i^{-1}(x-u_i) - (x-u_j)^T\\Sigma_j^{-1}(x-u_j)\\}-\\frac{1}{2}\\ln\\frac{|\\Sigma_i|}{|\\Sigma_j|} \\\\ = \u0026 -\\frac{1}{2}(x^T\\Sigma_i^{-1}x - x^T\\Sigma_j^{-1}x) + (u_i^T\\Sigma_i^{-1}-u_j^T\\Sigma_j^{-1})x-\\frac{1}{2}(u_i^T\\Sigma_i^{-1}u_i - u_j^T\\Sigma_j^{-1}u_j) \\\\ \u0026 - \\frac{1}{2}\\ln\\frac{|\\Sigma_i|}{|\\Sigma_j|} \\end{aligned} $$  显然，当$\\Sigma_i=\\Sigma_j$时，$g_{ij}$为线性判别函数，因此当各类条件概率密度函数中的协方差相同时，为线性判别函数。 2、解 (1) Parzen窗概率密度估计函数为 $$p(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp(-\\frac{(x-x_i)^2}{2\\sigma^2})$$ 当$\\sigma=0.5$时 $$p(x)=\\sqrt{\\frac{2}{\\pi}}\\exp(-2(x-x_i)^2)$$ 其中$x_i$为密度函数均值\n(2) 概率密度函数为\n $$p(x)=\\left\\{ \\begin{aligned} \\frac{1}{8|x-8|} \u0026 , \u0026 x6 \\\\ \\frac{1}{8|x-4|} \u0026 , \u0026 2曲线图为  3、解 (1) 算法步骤：\n  确定聚类个数$c$，随机选择$c$个聚类中心;\n  对每个样本点，选择与其欧式距离最近的聚类中心作为其类别代表，将具有相同类别代表的样本归为一类，划分为$c$个类，计算每个类的样本均值$m_i$，以$m_i$作为每类新的聚类中心，$i=1,\\dots,c$;\n  对于第$i$类的一个样本$x'$，计算将其放入$j$类之后，总的平方误差的减少量$I_{ij}$ $$I_{ij}=\\frac{n_i}{n_i-1}||x\u0026rsquo;-m_i||^2-\\frac{n_j}{n_j+1}||x\u0026rsquo;-m_j||^2$$ 令$I_{ik}=\\mathop{max}\\limits_j{I_{ij}}$，如果$I_{ik}\u0026gt;0$，则将$x'$从第$i$个类别移动到第$k$个类别，更新$i$类和$k$类均值 $$m_i \\leftarrow m_i-\\frac{1}{n_i-1}(x\u0026rsquo;-m_i)$$ $$m_k \\leftarrow m_k+\\frac{1}{n_k+1}(x\u0026rsquo;-m_k)$$ 否则不移动$x'$;\n  重复2.和3.过程，如果移动任何样本都不会减少总的平方误差，则算法结束。\n  (2) 假设将$x'$移动到$D_j$聚类之后，$D_j$聚类新的均值为\n$$m_j^*=m_j+\\frac{1}{n_j+1}(x'-m_j)$$ $D_j$聚类新的平方误差为 $$ \\begin{aligned} J_j^* =\u0026 \\sum_{x\\in D_j\\cup {x'}}||x-m_j^*||^2 \\\\ =\u0026 \\sum_{x\\in D_j}||x-m_j^*||^2+||x'-m_j^*||^2 \\\\ =\u0026 \\sum_{x\\in D_j}||x-m_j-\\frac{1}{n_j+1}(x'-m_j)||^2+||x'-m_j-\\frac{1}{n_j+1}(x'-m_j)||^2 \\\\ =\u0026 \\sum_{x\\in D_j}\\left\\{||x-m_j||^2 + ||\\frac{1}{n_j+1}(x'-m_j)||^2-\\frac{2(x-m_j)^T(x'-m_j)}{n_j+1}\\right\\} \\\\ \u0026 +\\frac{n_j^2}{(n_j+1)^2}||x'-m_j||^2 \\\\ =\u0026 J_j + \\frac{n_j}{(n_j+1)^2}||x'-m_j||^2+\\frac{n_j^2}{(n_j+1)^2}||x'-m_j||^2 \\\\ =\u0026 J_j + \\frac{n_j}{n_j+1}||x'-m_j||^2 \\end{aligned} $$ 4、解 (1) Adaboost算法步骤：\n 选择m个弱分类器$f_j(x)\\in {-1, 1}$, 如果$x$为第一类样本，则结果为$1$，否则结果为$-1$, $j=1,\\dots,m$； 初始化样本权重$w_i=\\frac{1}{n}$, $i=1,\\dots,n$； 令$j=1\\to m$，作如下操作：  使用$n$个由$w_i$加权的样本$x_i$构造分类器$f_j$，$i=1,\\dots,n$； 计算$f_j$在训练样本上的分类错误率$e_j$，令$c_j=log\\frac{1-e_j}{e_j}$. 如果$f_j$对$x_i$样本分类正确，则权重$w_i$不变，否则$w_i=w_i\\cdot exp(c_j)$，$i=1,\\dots,n$； 将$w_i$归一化，$i=1,\\dots,n$.   构造最终分类器$f(x)=\\mathrm{sgn}\\left[\\sum_{j=1}^m c_jf_j(x)\\right]$.  (2) 训练误差为0后，Adaboost会继续增大分类间距，提升模型的泛化能力，减少测试误差\n5、解 (1) K-L变换矩阵为\n $$ \\begin{aligned} \\Sigma =\u0026 E[x\\cdot x^T] \\\\ =\u0026 \\frac{1}{8}\\{(-4, 1)^T (-4, 1) + (-2, 1)^T (-2, 1) \\\\ =\u0026 (-4, -1)^T (-4, -1) + (-2, -1)^T (-2, -1) \\\\ =\u0026 (4, 1)^T (4, 1) + (2, 1)^T (2, 1) \\\\ =\u0026 (4, -1)^T (4, -1) + (2, -1)^T (2, -1)\\} \\\\ =\u0026 \\begin{gathered} \\begin{pmatrix} 10 \u0026 0 \\\\ 0 \u0026 1 \\end{pmatrix} \\end{gathered} \\end{aligned} $$  显然，$\\Sigma$的特征值为$10$和$1$，对应的特征向量分别为$(1, 0)^T$和$(0, 1)^T$. (2) 令两类集合分别为$w_1$和$w_2$, 各类样本均值分别为$m_1$和$m_2$，则\n $$ \\begin{aligned} \u0026 S_b=(m_1-m_2)(m_1-m_2)^T \\\\ \u0026 S_w=\\sum_{x\\in w_1}(x-m_1)(x-m_1)^T+\\sum_{x\\in w_2}(x-m_2)(x-m_2)^T \\end{aligned} $$  对第一个特征计算样本的$S_b$和$S_w$得 $$S_b=36, S_w=8$$ 因此$J_5=\\frac{11}{2}$ 对第二个特征计算样本的$S_b$和$S_w$得 $$S_b=0, S_w=8$$ 因此$J_5=0$. 由于$\\frac{11}{2}0$，所以应选择第一个特征进行分类任务。 6、解 (1) 原优化问题：\n $$ \\begin{aligned} \u0026 \\min_{W,b} \u0026 \\frac{1}{2}||W||^2+C\\sum_{i=1}^n \\xi_i\\\\ \u0026 s.t. \u0026 y_i(W^Tx_i+b)\\ge1-\\xi_i, \\\\ \u0026 \u0026 \\xi_i\\ge0, i=1,\\dots,n. \\end{aligned} $$  对偶优化问题：  $$ \\begin{aligned} \u0026 \\max_{\\alpha} \u0026 \\sum_{i=1}^n\\alpha_i-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^na_ia_jy_iy_jx_i^Tx_j\\\\ \u0026 s.t. \u0026 \\sum_{i=1}^na_iy_i=0, \\\\ \u0026 \u0026 0\\le\\alpha_i\\le C, i=1,\\dots,n. \\end{aligned} $$  (2) 当$C$很大时，分类超平面对错分样本比较敏感，因此要保证不错分的情况下，最大化最小间隔\n (3) 当$C$很小时，分类超平面对错分样本不敏感，而对$W$的长度比较敏感\n 7、解 (1) 假设输入层神经元个数为$d$个，每个神经元的输入为$x_i$，$i=1,\\dots,d$；隐藏层的神经元个数为$d_1$个，激活值分别为$h\u0026rsquo;_j$，输出分别为$h_j=\\sigma_1(h\u0026rsquo;_i)$，其中$\\sigma_1$为隐藏层的激活函数，$j=1,\\dots,d_1$；输出层的神经元个数为$c$个，激活值分别为$y\u0026rsquo;_k$，输出为$y_k=\\sigma_2(y\u0026rsquo;_k)$，其中$\\sigma_2$为输出层的激活函数，$k=1,\\dots,c$.\n输入层与隐藏层之间的参数为$w^{(1)}_{ij}$和$b^{(1)}_j$，满足：\n $$h'_j=\\sum_{i=1}^dw^{(1)}_{ij}x_i+b^{(1)}_j$$  输入层与隐藏层之间的参数为$w^{(2)}_{jk}$和$b^{(2)}_k$，满足：\n $$y'_k=\\sum_{j=1}^dw^{(2)}_{jk}h_j+b^{(2)}_k$$  令$L$为网络的损失函数，则有：\n $$\\left\\{ \\begin{aligned} \u0026 \\nabla_{w^{(2)}_{jk}}=\\sum_{k=1}^c\\frac{\\partial L}{\\partial y_k}\\frac{\\partial y_k}{\\partial y'_k}\\frac{\\partial y'_k}{\\partial w^{(2)}_{jk}} = \\sum_{k=1}^c\\frac{\\partial L}{\\partial y_k}\\sigma'_2(y'_k)h_j \\\\ \u0026 \\nabla_{b^{(2)}_k}=\\sum_{k=1}^c\\frac{\\partial L}{\\partial y_k}\\frac{\\partial y_k}{\\partial y'_k}\\frac{\\partial y'_k}{\\partial b^{(2)}_k} = \\sum_{k=1}^c\\frac{\\partial L}{\\partial y_k}\\sigma'_2(y'_k) \\\\ \u0026 \\nabla_{w^{(1)}_{ij}}=\\sum_{k=1}^c\\sum_{j=1}^{d_1}\\frac{\\partial L}{\\partial y_k}\\frac{\\partial y_k}{\\partial y'_k}\\frac{\\partial y'_k}{\\partial h_j}\\frac{\\partial h_j}{\\partial h'_j}\\frac{\\partial h'_j}{\\partial w^{(1)}_{ij}} = \\sum_{k=1}^c\\sum_{j=1}^{d_1}\\frac{\\partial L}{\\partial y_k}\\sigma'_2(y'_k)w^{(2)}_{jk}\\sigma'_1(h'_j)x_i\\\\ \u0026 \\nabla_{b^{(1)}_j}=\\sum_{k=1}^c\\sum_{j=1}^{d_1}\\frac{\\partial L}{\\partial y_k}\\frac{\\partial y_k}{\\partial y'_k}\\frac{\\partial y'_k}{\\partial h_j}\\frac{\\partial h_j}{\\partial h'_j}\\frac{\\partial h'_j}{\\partial b^{(1)}_j} = \\sum_{k=1}^c\\sum_{j=1}^{d_1}\\frac{\\partial L}{\\partial y_k}\\sigma'_2(y'_k)w^{(2)}_{jk}\\sigma'_1(h'_j) \\end{aligned} \\right. $$  网络经过前向传播之后，得到了计算梯度$\\nabla_{w^{(2)}_{jk}}$, $\\nabla_{b^{(2)}_k}$, $\\nabla_{w^{(1)}_{ij}}$, $\\nabla_{b^{(1)}_j}$所需的各个变量的值，因此可以利用反向传播依次求出它们的值，然后利用梯度下降对参数$w^{(2)}_{jk}$, $b^{(2)}_k$, $w^{(1)}_{ij}$, $b^{(1)}_j$进行更新：\n $$\\left\\{ \\begin{aligned} \u0026 w^{(2)}_{jk}\\gets w^{(2)}_{jk} - \\eta\\nabla_{w^{(2)}_{jk}} \\\\ \u0026 b^{(2)}_k\\gets b^{(2)}_k - \\eta\\nabla_{b^{(2)}_k} \\\\ \u0026 w^{(1)}_{ij}\\gets w^{(1)}_{ij} - \\eta\\nabla_{w^{(1)}_{ij}} \\\\ \u0026 b^{(1)}_j\\gets b^{(1)}_j - \\eta\\nabla_{b^{(1)}_j} \\end{aligned} \\right. $$  其中$\\eta$为学习率。  (2) 沿用(1)中的假设，计算步骤为：   初始化参数$w^{(2)}_{jk}$，$b^{(2)}_k$，$w^{(1)}_{ij}$, $b^{(1)}_j$;  利用前向传播过程得到输出，从输入$x_i$，得到$h_j$和$y_k$，$i=1,\\dots,d$，$j=1,\\dots,d_1$，$k=1,\\dots,c$；  计算样本标签和输出$y_k$之间的差异，即损失函数$L$；  参数更新，如果模型损失函数小于阈值，则结束。否则，利用(1)中公式更新参数$w^{(2)}_{jk}$, $b^{(2)}_k$, $w^{(1)}_{ij}$, $b^{(1)}_j$的值；  重复1,2,3,4，直至结束。   (3) 如果神经网络中的激活函数为Sigmoid函数，以$\\sigma_1$为例，即 $$ \\sigma_1(x)=\\frac{1}{1+\\exp(-x)} $$ 则  $$ \\sigma'_1(x)=\\frac{\\exp(-x)}{(1+\\exp(-x))^2}=\\sigma_1(x)(1-\\sigma_1(x)) $$  当x远离$0$时，$\\sigma'_1(x)\\to 0$，从而$\\nabla_{w^{(1)}_{ij}}\\to 0$, $\\nabla_{b^{(1)}_j}\\to 0$，这样参数$w^{(1)}_{ij}$, $b^{(1)}_j$在训练过程中得不到有效更新，因此出现麻痹现象。 8、解 (1) 深度学习定义：深度学习是机器学习算法的一种，通过创建多层神经网络模型，使其能够自动提取样本中的特征，并对样本进行分类或回归，采用反向传播算法对参数进行更新，从而得到对样本有效分类或回归的模型。\n(2)\n 卷积神经网络(CNN)：图像分类、目标检测； 递归神经网络(RNN)：机器翻译、人机对话； 全卷积神经网路：语义分割； 全连接神经网络：邮件分类  ","date":1584624958,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584624958,"objectID":"8ada89d3d7d6c8ceb43c9488462b6b18","permalink":"https://shishuai.org/post/2020/03/19/1/","publishdate":"2020-03-19T21:35:58+08:00","relpermalink":"/post/2020/03/19/1/","section":"post","summary":"点击这里下载题目，提取码: wbs4 1、解 (1) 令$g_{ij}(x)=\\ln(\\frac{p(x|w_i)}{p(x|w_j)})$$=\\ln p(x|w_i)-\\ln p(x","tags":["pattern recognition"],"title":"2018年中科院自动化所模式识别考博真题解答","type":"post"}]